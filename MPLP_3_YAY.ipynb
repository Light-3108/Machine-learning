{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3vor0sPQx7Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55hXgtspQ4yh",
        "outputId": "543e0e91-e3ab-45b3-c454-b8810d4e4d19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "words[:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hERTcb1_RCCL",
        "outputId": "799787db-7223-40c2-995a-f94384060bcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dN_dTdwREF3",
        "outputId": "2e435692-1a20-474f-c91b-f09b91a0d1e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ],
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJBu9WQNRKgI",
        "outputId": "f6d6b098-1a1a-4f31-d477-76c8e1da161a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12097\n"
          ]
        }
      ],
      "source": [
        "# MLP revisited\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n",
        "#b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
        "\n",
        "# BatchNorm parameters\n",
        "bngain = torch.ones((1, n_hidden))\n",
        "bnbias = torch.zeros((1, n_hidden))\n",
        "bnmean_running = torch.zeros((1, n_hidden))\n",
        "bnstd_running = torch.ones((1, n_hidden))\n",
        "\n",
        "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#aba ata kaiming init launu paryo forward pass normalize garna lai so\n",
        "#we are using tanh so\n",
        "(5/3) / (30**0.5)     #input size 30 hunxa ni ta 10 + 10 + 10\n",
        "#= 0.304. vanesi yesle multiply garnu paryo w1 lai\n",
        "\n",
        "\n",
        "# ***sike*****\n",
        "# Actually this kaiming init for  normalization is not important due to modern invotation\n",
        "# easy kei hola, teso vaye\n",
        "\n",
        "# what is that then?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64XZwDPTbjO2",
        "outputId": "749d99d0-dd30-42ef-9228-7c196569040f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3042903097250923"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#batch normalization comes into play\n",
        "#google research\n",
        "#idea: if you want your layer to be gussian, then just normalize the layer to make it gussian. simple as that, hehe\n",
        "#vanesi hamle hpreact lai normalize garnu paryo hoina? yes.\n",
        "# just do this.\n",
        "# 1. paile hpreat ko mean nikalne row bata. i.e 1 * 200 hune vayo.\n",
        "# 2. mathi ko jasari nai standard deviation nikalne row bata nai 1*200 nai vayo.\n",
        "# 3. now change hpreact to hpreact = (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=true)  normalize vayo\n",
        "\n",
        "# yo garepaxi k vayo ta?\n",
        "# aba hamro tyo batch i.e 32 examples ko sabai 200 neurons ko s.d 1 ko najik hunxa. which is good for network to learn\n",
        "# so it is called batch normalization. nice\n",
        "\n",
        "# mathi ko normalization is perfectly differentiable hence backpropagate garna sakne ni vayo ni ta easily. hehe\n",
        "\n",
        "\n",
        "\n",
        "# -- observations hamle mathi ko normalization ta herek iteration ma garko xa ni ta. meaning we are forcing model to make it's activation gussian\n",
        "# yesto garda chain khasai ramro learn garena xa.\n",
        "# k garnu paryo vanda kheri. initialization ma matra normalization garnu paryo.   My idea:  epoch epoch or testai time interval ma normalization garda kasto hola ra?\n",
        "\n",
        "\n",
        "# vanesi shifting and adding bias comes into play\n",
        "# introduce bngain as torch.ones((1,n_hidden))\n",
        "# introdue bnbias as torch.zeros((1,n_hidden))\n",
        "\n",
        "# now change hpreact to hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=true) + bnbias\n",
        "# yeti garsi ta first ma unit gussian nai hunxa initialization ma ta\n",
        "# ani hamle yo formula ta forward pass ma nai rekhxum ni ta so aba model is free to use this to decrease the loss.\n",
        "\n",
        "# Q. ok first ma ta unit gussian lai gareko ho. aba training hudai garda chai yo change le kasari help garyo hamlai, yo chai exactly bujina maile.\n",
        "# Q. herek choti normalize ta hunxa tara network le gain ra shift ni garna payo hoina?\n",
        "# Q. I still think this is some how limiting the network, kinaki normalization ta still gariraxum ni ta hamle.\n",
        "# -> normalization lai counter garna milxa ta bgain ra bbias le, so farak pardaina ra network ko aafno loss decrease garna jasari ni\n",
        "# -> parameters twiks garna sakxa, ata tyo fredom xa. (always normaization hunxa vanne xaina ni ta bgain ra bbias vayeko le)\n",
        "\n",
        "#47:30\n",
        "\n",
        "\n",
        "# ****************************** But there is some problem in this batch-normalization method ****************************\n",
        "\n",
        "# 1. we know hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=true) + bnbias\n",
        "# 2. yo ta forward pass ra backward pass dubai ma aauxa sure\n",
        "# 3. aba euta example ko activation nikanla batch ko sabai example ko use hune vayo ni ta, right? kina ki, hpreact.mean  ra hpreact.std garda ta sabai use hunxa\n",
        "#    ni ta\n",
        "# 4. this is terrifying scientists.\n",
        "# 5. But it turns out yesle khasai hamper garne rainxa, yesle chai regulariser jasari use hune raixa\n",
        "\n",
        "# is there any normalization jasma chai yo problem hudain?\n",
        "\n",
        "\n",
        "# another problem, aba hamle next char kasari predict garne ra?\n",
        "# kina ki ata ta forward pass ma ta hpreact ta use garnai parxa, ani hpreact ma ta .mean() use vako xa jasko lagi batch chainxa.\n",
        "# k garne vanda global mean ra global std deviation use garne\n",
        "\n",
        "# -> (yo different block of code ma ni nikalna milyo, tara running ma ni nikalnu milne raixa, k k garera.)\n",
        "\n",
        "# aba yo ta constant jasto vayo ni ta variable ta vayena ni ta.\n",
        "\n",
        "# Q1. yo global mean ra global standard deviation training phase ma use garna milxa?\n",
        "# Q2. yo use garda kei hamper hunna ra? yeslai sochnu parla ali yar\n",
        "\n"
      ],
      "metadata": {
        "id": "QmWxxVE3pF8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet - yo chai image classification ma kaam aauxa, image input hunxa ra blocks of neural network hunxa, ra output ma chai image classification hunxa\n",
        "# yo bock haru lai chain bottelneck block vanne raixa.\n",
        "# yesko implementation github ma easily pauxa so take a look at it."
      ],
      "metadata": {
        "id": "1jLzq_-D5rln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1000,10) #1000*10\n",
        "w = torch.randn(10,200) * (1/10**0.5)#10*200\n",
        "\n",
        "print(x.mean(),x.std())   #ok yo random le normal distribution dine raixa ni ta\n",
        "\n",
        "y = x @ w\n",
        "print(y.mean(),y.std())    #ok yesko mean ni zero ma nai aaune raixa, tara standard deviation chai ali 1 vanda thulo hune raixa.\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.subplot(121)\n",
        "plt.hist(x.view(-1).tolist(),50);\n",
        "plt.subplot(122)\n",
        "plt.hist(y.view(-1).tolist(),50);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "B5z-7BA9RE-0",
        "outputId": "1f9d18f8-b491-42f2-9e77-9a68a6c68722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0160) tensor(0.9969)\n",
            "tensor(-0.0012) tensor(0.9855)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  ata we can see, y = x @ w # + b.  yesto garepaxi ta s.d increase hune raixa hoina 1 bata ali badi like 3.\n",
        "# andrej le vaneko activation haru ko s.d 1 ko oripari vayko ramro re. why?\n",
        "# hamro y ni ta activation ta ho ni ata. so yeslai k garne?\n",
        "# can we manupulate w to get the result?\n",
        "# -yes w lai ni manupulate garne raixa, tara, w lai jasti 0.1 yesto haru le multiply garda information loss hunna?\n",
        "\n",
        "# hame w lai large number le multiply garda y ko s.d ekdam thulo hune raixa, sano le garda sano hune raixa.\n",
        "\n",
        "# w ma kati le multiply garne? jasle chai s.d takkai 1 layeos?\n",
        "# w lai sqrt of (row) le garda hune raixa.\n",
        "\n",
        "# yo chain activation funtion anusar ni hune raixa. jun chai euta research paper le lekheko xa.\n",
        "# tesma k vaneko xa vanda kheri hamle forward pass ni normalzie garna sakyo backward pass ni normalize garna sakyo.\n",
        "# tara it don't matters actually kina ki forward normalize vayepaxi backward ni normalize hune raixa.\n",
        "\n",
        "# relu xa vane root 2/ root 10 garnu parxa. kina root 2 ? kina ki relu le negative ko ta sabai squacch gardninxa ni ta so * 2 gareko\n",
        "# testai tanh ma chai 5/3 le multiply hune raixa. same reson. squash garxa ni ta. so weights lai boost deko.\n",
        "\n",
        "# we call it gain, gain = 1, root 2 , 5.3 and so on aru ko ni.\n",
        "# yo ramro sanga bujeko xaina\n",
        "# 1. initialzation ma sabai activations hau normalize nai kina huna parne?\n",
        "# 2. forward pass normalize vayepaxi backward pass ni kasari normalize hunxa?\n",
        "# 3. yo gain haru actually nai ati vanera kasari thapa ko?\n",
        "\n",
        "# pytorch ma xa kaiming init vaera yesko lagi"
      ],
      "metadata": {
        "id": "WARpBP9rUDXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUFy-ZpRRk14",
        "outputId": "467c6e25-1374-40b5-d3f6-a8559d8a212c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.3239\n",
            "  10000/ 200000: 2.0322\n",
            "  20000/ 200000: 2.5675\n",
            "  30000/ 200000: 2.0125\n",
            "  40000/ 200000: 2.2446\n",
            "  50000/ 200000: 1.8897\n",
            "  60000/ 200000: 2.0785\n",
            "  70000/ 200000: 2.3681\n",
            "  80000/ 200000: 2.2918\n",
            "  90000/ 200000: 2.0238\n",
            " 100000/ 200000: 2.3673\n",
            " 110000/ 200000: 2.3132\n",
            " 120000/ 200000: 1.6414\n",
            " 130000/ 200000: 1.9311\n",
            " 140000/ 200000: 2.2231\n",
            " 150000/ 200000: 2.0027\n",
            " 160000/ 200000: 2.0997\n",
            " 170000/ 200000: 2.4949\n",
            " 180000/ 200000: 2.0199\n",
            " 190000/ 200000: 2.1707\n"
          ]
        }
      ],
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  # Linear layer\n",
        "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation,   batch normalization ma nai bias xa ni ta so we don't use bias here.\n",
        "  # BatchNorm layer\n",
        "  # -------------------------------------------------------------\n",
        "  bnmeani = hpreact.mean(0, keepdim=True)\n",
        "  bnstdi = hpreact.std(0, keepdim=True)\n",
        "  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
        "  with torch.no_grad():\n",
        "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
        "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
        "  # -------------------------------------------------------------\n",
        "  # Non-linearity\n",
        "  h = torch.tanh(hpreact) # hidden layer\n",
        "  logits = h @ W2 + b2 # output layer\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#global mean ra sd nikalna (running ma hoina)\n",
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 # + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnstd = hpreact.std(0, keepdim=True)"
      ],
      "metadata": {
        "id": "btsJg3l2Ylym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "OpqOqm_MSJpB",
        "outputId": "b1cf0198-5c00-472f-e020-ee71273915ea"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'b1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-649d6f98d6fc>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0msplit_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0msplit_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-649d6f98d6fc>\u001b[0m in \u001b[0;36msplit_loss\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# (N, block_size, n_embd)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0membcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# concat into (N, block_size * n_embd)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mhpreact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membcat\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mW1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0;31m# hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mhpreact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbngain\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhpreact\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbnmean_running\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbnstd_running\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbnbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'b1' is not defined"
          ]
        }
      ],
      "source": [
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
        "  hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lX2I1QniYNqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEm2p8LvTZwR"
      },
      "outputs": [],
      "source": [
        "# hamro paile nai initialization ma nai ekdam thulo loss aayo ni ta, aba yeslai k ganre?\n",
        "# b2 lai zero banaune ani w2 lai ni zero ko close ma lane. yeso vayepaxi logits sabai eqauali i.e 1 hunxa ra loss kam aauxa\n",
        "# hamle paile nai yesari initialize garepaxi graph herda testo paile ko jasto first ma nai rapidly decrease hunthyo ni ta ho\n",
        "\n",
        "# tyo chai hudaina. thorai decrease vayejasto hunxa, tara same or better error aauxa paile ko vanda chain.\n",
        "# yesle k bujayo vanda paile parameters harulai zero tira squash garne raixa loss funtion le which is the easy part. tesaile loss ekdam rapidly decrease\n",
        "# vayeko jasto dekhinxa. now aba actually training hunxa ni platue jasto ma aaepaxi.\n",
        "\n",
        "# ok this problem is solved. but we have the problem in h.\n",
        "# 1. what is the problem?\n",
        "# 2. why this is affecting the loss\n",
        "# 3. what is the solution\n",
        "\n",
        "# 13:50\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIy8TwAVMnDo",
        "outputId": "0e85e610-22f5-44d7-d43b-4917e3b1a63b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 200])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "h.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "jbUF5gkaMruo",
        "outputId": "9211d774-7c17-41ca-946a-02af6bd1957b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.hist(h.view(-1).tolist(),50);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liAiFs5cNGGz"
      },
      "outputs": [],
      "source": [
        "#oh here we see most of the values because -1 or +1 when we do tanh. hence i think here we loss many information. because\n",
        "#2 is also converted 1 and 1000 is also coverted to 1. but they shouldn't have same contribution for forward pass off-course.\n",
        "# backward propagation ma (1 - t^2) * out.grad hunxa ni ta. no aba t = 1 vaye ta zero vayo ni ta. so loss ma ni kei contribute garena"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOBzokZON9qa"
      },
      "outputs": [],
      "source": [
        "plt.hist(hpreact.view(-1).tolist(),50);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5loo0MMDN7-Q"
      },
      "outputs": [],
      "source": [
        "#tanh nagardai ta dami xani ta. tara tanh garesi katti information loss vayo hai?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7i5n7xSQOlIV"
      },
      "outputs": [],
      "source": [
        "# now what is the solution? here?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW6J0vyHQq8n"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.imshow(h.abs() > 0.99, cmap='gray', interpolation='nearest')\n",
        "#true xa vane white aauxa. 32 ota example ma nai herum hai ta kata kata chai > 0.99 aauxa, i.e flat region ma parxa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-ZGkpZJRJQZ"
      },
      "outputs": [],
      "source": [
        "# #ekdam badi thaun ma >0.99 xa.  vanesi yetro thaun gradient distroyed hunxa.\n",
        "# #same case for other activation functions like sigmoid, tanh, reLU etc.\n",
        "# relu ma yesto pani huna sakxa jastai training garda gardai activation function zero vayo relu le garda, now aba learn ta gardaina yesle\n",
        "# aba sabai training example harule ni teslai active nabaune sakxa. which indicates it is a dead neuron.\n",
        "# But why is this issue? I claim is that is a dead neuron then it is not important to decrease the loss. (I may be wrong, no i am 100% wrong, but don't know why)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfXJzIwmUPcc"
      },
      "outputs": [],
      "source": [
        "#My solution. let's squash the above figure between -1 and 1.\n",
        "# i.e change hpreact offcourse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5FFhyWkUX69"
      },
      "outputs": [],
      "source": [
        "#Andrej le W1 lai 0.1 ma change garyo ani hpreact ta -1 to 1 ko range ma aayo ta how?\n",
        "# paile ta yo sabai graphs haru initialization ko bela ko ho. hamle initialization ma nai ramro garum na ani learn ramro sanga huna sakxa vanne ho\n",
        "# so W1 lai 0.1 garepaxi ta. 200 activation haru -1 to 1 ma nai aauxa because *0.1 gareasi sano hunxa ni ta.\n",
        "# praye chai -1 to 1 ma nai aauxa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqKyEwu9Y88w"
      },
      "outputs": [],
      "source": [
        "#ok white nai xaina ta. tara thorai ta aauna parne hoina ra?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6LQ3k1PVz04"
      },
      "outputs": [],
      "source": [
        "# # loss log\n",
        "# # original:\n",
        "# train 2.124\n",
        "# val 2.16\n",
        "\n",
        "# #fix softmax confidently wrong(first ma ta sabai ko probability same hunu parthyo ni ta)\n",
        "# train 2.09\n",
        "# val 2.102\n",
        "\n",
        "# #fix tanh layer too saturated at init:\n",
        "# train 2.0355\n",
        "# val 2.1026"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAjI0sWCbvha"
      },
      "outputs": [],
      "source": [
        "#hamro yo ta ekdam sano neural network ho ni ta, so initialzation ma hamle khasai focus nagare ni learn garyo network le thikai\n",
        "#but this is not the case when the network is super large.\n",
        "#so this is very imp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAjmtcbocfFe"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ********Summary + pytorchyfying********\n",
        "# batch normalization are not good, so try to avoid them.\n",
        "# group normalization, layer normalization are good.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ls-4SF5bFaZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's train a deeper network\n",
        "# The classes we create here are the same API as nn.Module in PyTorch\n",
        "\n",
        "class Linear:\n",
        "\n",
        "  def __init__(self, fan_in, fan_out, bias=True):\n",
        "    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5     # harek input ko each output ko lagi weights chaiyo ni ta.\n",
        "    self.bias = torch.zeros(fan_out) if bias else None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = x @ self.weight\n",
        "    if self.bias is not None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "\n",
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    # parameters (trained with backprop)\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "    # buffers (trained with a running 'momentum update')\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    if self.training:\n",
        "      xmean = x.mean(0, keepdim=True) # batch mean\n",
        "      xvar = x.var(0, keepdim=True) # batch variance\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    # update the buffers\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "class Tanh:\n",
        "  def __call__(self, x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "\n",
        "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "layers = [\n",
        "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
        "]\n",
        "# layers = [\n",
        "#   Linear(n_embd * block_size, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, vocab_size),\n",
        "# ]\n",
        "\n",
        "with torch.no_grad():\n",
        "  # last layer: make less confident\n",
        "  layers[-1].gamma *= 0.1\n",
        "  #layers[-1].weight *= 0.1\n",
        "  # all other layers: apply gain\n",
        "  for layer in layers[:-1]:\n",
        "    if isinstance(layer, Linear):\n",
        "      layer.weight *= 1.0 #5/3\n",
        "\n",
        "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "Nyt58dAIRUiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. yo fan_in ** 0.5 kina gareko re?\n",
        "# -> yo s.d 1 ra mean zero banauna hoina? kamit initialization\n",
        "# self.out += self.bias xa, easy.  bias add gareko ta honi\n",
        "# .paremeters call garda array ma wights ra bias bharera dinxa\n",
        "\n",
        "\n",
        "# batch normalization ma k k xa ta?\n",
        "# easy kunai layer lai batch norm, i.e normalize garera pathauxa. easy\n",
        "\n",
        "# self.out (batch norm ko) ma chain normalize garisakepaxi ko layers ko activation haru raixa hoina? yes\n",
        "\n",
        "# yo return [self.gamma, self.beta]  yo return chain kina gareko? batch norm ko parameters ho yeni haru?\n",
        "\n",
        "\n",
        "# Linear(100,200), yo vanna le k bujne? 100 activations connected vako xa 200 activations ma, different layers ko? yes"
      ],
      "metadata": {
        "id": "aaL6vdDi_Eel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "ud = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  for layer in layers:\n",
        "    x = layer(x)\n",
        "  loss = F.cross_entropy(x, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for layer in layers:\n",
        "    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "  with torch.no_grad():\n",
        "    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
        "\n",
        "  if i >= 1000:\n",
        "    break # AFTER_DEBUG: would take out obviously to run full optimization"
      ],
      "metadata": {
        "id": "35BtGX2RRVyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize histograms\n",
        "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
        "legends = []\n",
        "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
        "  if isinstance(layer, Tanh):\n",
        "    t = layer.out\n",
        "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
        "    hy, hx = torch.histogram(t, density=True)\n",
        "    plt.plot(hx[:-1].detach(), hy.detach())\n",
        "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
        "plt.legend(legends);\n",
        "plt.title('activation distribution')"
      ],
      "metadata": {
        "id": "4qIsxiFfTOmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. yo -1 to 1 vaneko ta layers ko activation hola hoina,\n",
        "# ani yo y axix ma chain k ho ra? is the specify how many activation are there with specific activations. tara yo ta feri 0.2, 0.4 haru ma xa. tei nai ho.\n",
        "# Q. ani yo upward C jasto graph kina aayo ra? how is this good?\n",
        "# Q. yo saturated vaneko kati chai > 1 or < -1 xa vanera hoina?\n",
        "\n",
        "\n",
        "# hamle mathi sabai layer haru lai gain le multiply gareko xani i.e 5/3 le.\n",
        "# if hamle 1 le multiply gareko vaye, yo mathi ko graph farak aauthyo. upward C vanda ni downward C, jasto hunthyo\n",
        "# i.e sabai activation haru zero ko najik aauthyo\n",
        "# Q. yesto kina hola ra?\n",
        "# -> *1 garesi ta *5/3 gareko vanda saturation kam aaula.\n",
        "# -> originally chai zero ko najik jane tendency hunxa hola, tei vayera hamle (5/3 i.e >1) le multiply gareko.  -- my ans may be wrong\n",
        "\n",
        "# ---->linear layer lai ta hamle tanh garxum ni ta, but tanh le chai zero ko najik squach garne raixa\n",
        "# so hamle every time gain le multiply garnu paryo tyo hatauna  (nice by andrej)\n",
        "\n",
        "\n",
        "\n",
        "# so aba hamle ekdam thulo number le multiply garyo vane like 10. saturation ekdam badxa, offcourse\n"
      ],
      "metadata": {
        "id": "Ded5fUly4MzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize histograms\n",
        "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
        "legends = []\n",
        "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
        "  if isinstance(layer, Tanh):\n",
        "    t = layer.out.grad\n",
        "    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n",
        "    hy, hx = torch.histogram(t, density=True)\n",
        "    plt.plot(hx[:-1].detach(), hy.detach())\n",
        "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
        "plt.legend(legends);\n",
        "plt.title('gradient distribution')"
      ],
      "metadata": {
        "id": "TZlonoOmTQcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q. what the fuck is this?\n",
        "# yo every layer ko neurons ko gradient ko histogram ho yo. (nice)\n",
        "# lau gradient ko mean kasari zero vayo ra? wtf? ki maile ulto bujeko?\n",
        "\n",
        "# ani gradient zero bako badi xa ta. vanesi kasari learn garne ra? learning slow vayena ra?\n",
        "# ani yo saba layer ko gradient ko graph kasari same aauxa ra?   I am understanding something wrong here"
      ],
      "metadata": {
        "id": "Hk27wBuH-KQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Investing the graphs if there is  no tanh unit on the linear layers"
      ],
      "metadata": {
        "id": "HcRQZYhOAO13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize histograms\n",
        "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
        "legends = []\n",
        "for i,p in enumerate(parameters):\n",
        "  t = p.grad\n",
        "  if p.ndim == 2:\n",
        "    print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n",
        "    hy, hx = torch.histogram(t, density=True)\n",
        "    plt.plot(hx[:-1].detach(), hy.detach())\n",
        "    legends.append(f'{i} {tuple(p.shape)}')\n",
        "plt.legend(legends)\n",
        "plt.title('weights gradient distribution');"
      ],
      "metadata": {
        "id": "ABEdOTBKTSxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 4))\n",
        "legends = []\n",
        "for i,p in enumerate(parameters):\n",
        "  if p.ndim == 2:\n",
        "    plt.plot([ud[j][i] for j in range(len(ud))])\n",
        "    legends.append('param %d' % i)\n",
        "plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
        "plt.legend(legends);"
      ],
      "metadata": {
        "id": "430pCfRzTU-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  for layer in layers:\n",
        "    x = layer(x)\n",
        "  loss = F.cross_entropy(x, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "# put layers into eval mode\n",
        "for layer in layers:\n",
        "  layer.training = False\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "id": "Xp3RyMGETW7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass the neural net\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n",
        "      x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "      for layer in layers:\n",
        "        x = layer(x)\n",
        "      logits = x\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      # sample from the distribution\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      # shift the context window and track the samples\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      # if we sample the special '.' token, break\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out)) # decode and print the generated word"
      ],
      "metadata": {
        "id": "Tm5MByuxTZW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([1,2,3] + [4,5,6])"
      ],
      "metadata": {
        "id": "EjKLpingTcY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y5X8UK9RGxwS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}